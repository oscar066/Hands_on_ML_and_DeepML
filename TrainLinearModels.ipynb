{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import  numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as  sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression formula: $y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n$\n",
    "<ul>\n",
    "ŷ is the predicted value.\n",
    "<li>n is the number of features.\n",
    "<li>xi is the ith feature value.\n",
    "<li>θj is the jth model parameter (including the bias term θ0 and the feature weights θ1, θ2, ⋯, θn).</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the value of θ that minimizes the cost function, there is a closed-form solution —in other words, a mathematical equation that gives the result directly. This is called the Normal Equation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.12342866],\n",
       "       [2.84357955]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance \n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let’s compute θ using the Normal Equation. We will use the inv() function from NumPy’s Linear Algebra module (np.linalg) to compute the inverse of a matrix, and the dot() method for matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.12342866],\n",
       "       [9.81058776]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code computes the predicted target values (y_predict) for two new instances with feature values of 0 and 2, respectively, using a trained linear regression model that has been fit on a dataset with a single feature (X) and corresponding target values (y). The np.c_[] function is used to add the bias term to the new instances before making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.12342866] [[2.84357955]] \n",
      "\n",
      "[[4.12342866]\n",
      " [9.81058776]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "print(lin_reg.intercept_, lin_reg.coef_  ,'\\n')\n",
    "print(lin_reg.predict(X_new))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression class was based on the Normal Equation. This was an error, my apologies: as explained above, it is based on the pseudoinverse, which ultimately relies on the SVD matrix decomposition of X. The pseudoinverse is more efficient than the Normal Equation, especially when the number of features is large. The LinearRegression class is based on the pseudoinverse, not the Normal Equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32.55922417],\n",
       "       [60.99501968]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict([[10], [20]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Normal Equation computes the inverse of XT · X, which is an n × n matrix (where n is the number of features). The computational complexity of inverting such a matrix is typically about O(n2.4) to O(n3) (depending on the implementation). In other words, if you double the number of features, you multiply the computation time by roughly 22.4 = 5.3 to 23 = 8.\n",
    "The Normal Equation gets very slow when the number of features grows large (e.g., 100,000).\n",
    "On the positive side, this equation is linear with regards to the number of instances in the training set (it is O(m)), so it handles large training sets efficiently, provided they can fit in memory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely, you start by filling θ with random values (this is called random initializa‐ tion), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum<br>\n",
    "\n",
    "learning rate defines the size of steps to be taken by the algorithmn should be careful not to choose a very low one that would take a long time to converge neither a very large one \n",
    "that would make the algorithm diverge."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Two main challenges with Gradient Descent: if the ran‐ dom initialization starts the algorithm on the left, then it will converge to a local mini‐ mum, which is not as good as the global minimum. If it starts on the right, then it will take a very long time to cross the plateau, and if you stop too early you will never reach the global minimum.<br>\n",
    "\n",
    " Its important to ensure that all features have a similar scale (e.g., using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.12342866],\n",
       "       [2.84357955]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"font-size: 25px; color: red;\">Batch Gradient Descent</b><br>\n",
    "The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs batch gradient descent, a type of optimization algorithm commonly used in machine learning\n",
    "# to minimize a cost function\n",
    "\n",
    "def batch_gradient_descent(X, y, theta, eta, n_iterations):\n",
    "    \"\"\" \n",
    "    Performs batch gradient descent to minimize a cost function\n",
    "    X: array of training examples\n",
    "    y: array of target values\n",
    "    theta: array of model parameters\n",
    "    eta: learning rate\n",
    "    n_iterations: number of iterations to perform\n",
    "    \"\"\"\n",
    "    # Calculate the number of training examples in the dataset\n",
    "    m = len(y)\n",
    "    # Loop through the specified number of iterations\n",
    "    for iteration in range(n_iterations):\n",
    "        # Calculate the gradients of the cost function\n",
    "        gradients = 2/m * X.T.dot(X.dot(theta) - y)\n",
    "        # Update the model parameters using the gradients and the learning rate\n",
    "        theta = theta - eta * gradients\n",
    "    # Return the final values of the model parameters\n",
    "    return theta\n",
    "\n",
    "# theta_batch = batch_gradient_descent(X_b, y, theta, eta, n_iterations)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"font-size: 25px; color: red;\">Stochastic Gradient Descent</b><br>\n",
    "At each step, instead of computing the gradients based on the full training set (as in Batch Gradient Descent), the algorithm just picks a random instance in the training set and computes the gradients based only on that single instance. This makes the algorithm much faster since it has very little data to manipulate at every iteration. As a result, this algorithm is well suited for online learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of Stochastic Gradient Descent from scratch\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1) # random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs stochastic gradient descent, a type of optimization algorithm commonly used in machine learning\n",
    "# to minimize a cost function\n",
    "\n",
    "def stochastic_gradient_descent(X, y, theta, n_epochs, t0, t1):\n",
    "    \"\"\" \n",
    "    Performs stochastic gradient descent to minimize a cost function\n",
    "    X: array of training examples\n",
    "    y: array of target values\n",
    "    theta: array of model parameters\n",
    "    n_epochs: number of epochs to perform\n",
    "    t0: learning schedule hyperparameter\n",
    "    t1: learning schedule hyperparameter\n",
    "    \"\"\"\n",
    "    # Calculate the number of training examples in the dataset\n",
    "    m = len(y)\n",
    "    # Loop through the specified number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Loop through each training example\n",
    "        for i in range(m):\n",
    "            # Randomly select a training example\n",
    "            random_index = np.random.randint(m)\n",
    "            # Get the features and target value for the selected example\n",
    "            xi = X[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            # Calculate the gradients of the cost function using the selected example\n",
    "            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "            # Calculate the learning rate using a learning schedule\n",
    "            eta = learning_schedule(epoch * m + i, t0, t1)\n",
    "            # Update the model parameters using the gradients and the learning rate\n",
    "            theta = theta - eta * gradients\n",
    "    # Return the final values of the model parameters\n",
    "    return theta\n",
    "\n",
    "# theta_stochastic = stochastic_gradient_descent(X, y, theta, n_epochs, t0, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing stochastic gradient descent using sklearn\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(n_iters=50, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sgd_reg.intercept_, sgd_reg.coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"font-size: 25px; color: red;\">Mini-batch Gradient Descent</b><br>\n",
    "The last Gradient Descent algorithm we will look at is called Mini-batch Gradient Descent. It is quite simple to understand: at each step, instead of computing the gradients based on the full training set (as in Batch Gradient Descent), or based on just one instance (as in Stochastic Gradient Descent), the algorithm computes the gradients on small random sets of instances called mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_mgd = []\n",
    "\n",
    "n_iterations = 50\n",
    "minibatch_size = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "t0, t1 = 200, 1000\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs mini-batch gradient descent, a type of optimization algorithm commonly used in machine learning\n",
    "# to minimize a cost function\n",
    "\n",
    "def mini_batch_gradient_descent(X, y, theta, n_iterations, eta, minibatch_size):\n",
    "    \"\"\" \n",
    "    Performs mini-batch gradient descent to minimize a cost function\n",
    "    X: array of training examples\n",
    "    y: array of target values\n",
    "    theta: array of model parameters\n",
    "    n_iterations: number of iterations to perform\n",
    "    eta: learning rate\n",
    "    minibatch_size: size of the mini-batches to use\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the number of training examples in the dataset\n",
    "    m = len(y)\n",
    "    t = 0\n",
    "    # Loop through the specified number of iterations\n",
    "    for epoch in range(n_iterations):\n",
    "        # Shuffle the training data to prevent the algorithm from getting stuck in local minima\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        X_b_shuffled = X_b[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        # Loop through each mini-batch\n",
    "        for i in range(0, m, minibatch_size):\n",
    "            # Increment the iteration counter\n",
    "            t += 1\n",
    "            # Get the features and target values for the current mini-batch\n",
    "            xi = X_b_shuffled[i:i+minibatch_size]\n",
    "            yi = y_shuffled[i:i+minibatch_size]\n",
    "            # Calculate the gradients of the cost function using the current mini-batch\n",
    "            gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "            # Calculate the learning rate using a learning schedule\n",
    "            eta = learning_schedule(t)\n",
    "            # Update the model parameters using the gradients and the learning rate\n",
    "            theta = theta - eta * gradients\n",
    "    # Return the final values of the model parameters\n",
    "    return theta\n",
    "\n",
    "# theta_mini_batch = mini_batch_gradient_descent(X_b, y, theta, n_iterations, eta, minibatch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b5221f7fd568601ffb692ded55cc6a8f9f720c6422993e13b3964f3c2d5ea0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
